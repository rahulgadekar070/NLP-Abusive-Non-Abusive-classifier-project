# -*- coding: utf-8 -*-
"""Team-3 NLP model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nAVwoOVKmK70DeAyBfPR-ecsxBbMrAHB
"""

import pandas as pd
import re 
from nltk.tokenize import RegexpTokenizer
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.externals import joblib
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix
import seaborn as sns
sns.set()
import matplotlib.pyplot as plt
import numpy as np
import wordcloud
from wordcloud import WordCloud

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
stop_words = set(stopwords.words("english"))
default_stemmer = PorterStemmer()
default_stopwords = stopwords.words('english')

default_tokenizer=RegexpTokenizer(r"\w+")

from google.colab import drive
drive.mount('/content/drive')

email = pd.read_csv('/content/drive/My Drive/#Data Science/50k_emails')

email.head()

email.shape

email.columns

email.isnull().sum()

"""# **EDA & Feature Engineering**"""

email.drop(['Unnamed: 0', 'filename', 'Message-ID'],axis=1,inplace=True)

email.groupby('Class').describe().T

email['label'] = email['Class'].map({'Non Abusive': 0, 'Abusive': 1})     # add 'label' column

sns.set_style('whitegrid')
sns.countplot(email['Class'])
plt.title('Distribution of Abusive and Non_Abusive')

# Make a new column to show the length of content messages
email['length'] = email['content'].apply(len)

import plotly.express as px
fig = px.violin(data_frame=email, y="length", points="all", color="Class", 
                width=800, height=600)
fig.show()
# it shows median length of Abusive mail = 591.5 & Non_aAbusive = 735

colors = ['#ff9999','#66b3ff']
email["Class"].value_counts().plot(kind = 'pie',colors = colors ,explode = (0.1,0),autopct = '%1.1f%%')

from collections import Counter
count1 = Counter(" ".join(email[email['Class']=='Non Abusive']["content"]).split()).most_common(30)
df1 = pd.DataFrame.from_dict(count1)
df1 = df1.rename(columns={0: "words in Non-Abusive", 1 : "count"})

count2 = Counter(" ".join(email[email['Class']=='Abusive']["content"]).split()).most_common(30)
df2 = pd.DataFrame.from_dict(count2)
df2 = df2.rename(columns={0: "words in Abusive", 1 : "count_"})

df1.plot.bar(legend = False)
y_pos = np.arange(len(df1["words in Non-Abusive"]))
plt.xticks(y_pos, df1["words in Non-Abusive"])
plt.title('More frequent words in Non-Abusive messages')
plt.xlabel('words')
plt.ylabel('number')
plt.show()

df2.plot.bar(legend = False, color = 'orange')
y_pos = np.arange(len(df2["words in Abusive"]))
plt.xticks(y_pos, df2["words in Abusive"])
plt.title('More frequent words in Abusive messages')
plt.xlabel('words')
plt.ylabel('number')
plt.show()

"""# **Data Preprocessing**"""

### We can see that the majority of frequent words in both classes are stop words such as 'to', 'a', 'or' and so on...
### so we remove these stopwords by putting them into 'WordCloud'

Abusive_data = email[email["Class"] == "Abusive"]
Non_Abusive_data = email[email["Class"] == "Non Abusive"]

"""Wordcloud of Abusive & Non_Abusive class data"""

def show_wordcloud(data_Abusive_or_Non_Abusive, title):
    text = ' '.join(data_Abusive_or_Non_Abusive['content'].astype(str).tolist())
    stopwords = set(wordcloud.STOPWORDS)
    
    cloud = WordCloud(stopwords=stopwords,background_color='lightgrey',
                    colormap='viridis').generate(text)
    
    plt.imshow(cloud)  
    plt.axis('off')
    plt.title(title, fontsize=20 )
    plt.show()

show_wordcloud(Non_Abusive_data, "Non_Abusive messages")

show_wordcloud(Abusive_data, "Abusive messages")

"""**Data Cleaning**"""

stopword = nltk.corpus.stopwords.words('english')
unword = ['excelr', 'td', 'hou', 'cc', 'ect', 'subject', 'thi', 'asp', 'wa', 'fw', 'et', 'email', 'pm','ha']
stopword.extend(unword)

def clean_text(text, ):
        if text is not None:
        #exclusions = ['RE:', 'Re:', 're:']
        #exclusions = '|'.join(exclusions)
                text =  text.lower()
                text = re.sub('re:', '', text)
                text = re.sub('-', '', text)
                text = re.sub('_', '', text)
                text = re.sub(r'^https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
                text = re.sub(r'\S*@\S*\s?', '', text, flags=re.MULTILINE)
        # Remove text between square brackets
                text =re.sub('\[[^]]*\]', '', text)
        # removes punctuation
                text = re.sub(r'[^\w\s]','',text)
                text = re.sub(r'\n',' ',text)
                text = re.sub(r'[0-9]+','',text)
        # strip html 
                p = re.compile(r'<.*?>')
                text = re.sub(r"\'ve", " have ", text)
                text = re.sub(r"can't", "cannot ", text)
                text = re.sub(r"n't", " not ", text)
                text = re.sub(r"I'm", "I am", text)
                text = re.sub(r" m ", " am ", text)
                text = re.sub(r"\'re", " are ", text)
                text = re.sub(r"\'d", " would ", text)
                text = re.sub(r"\'ll", " will ", text)
        
                text = p.sub('', text)

        def tokenize_text(text,tokenizer=default_tokenizer):
            token = default_tokenizer.tokenize(text)
            return token
        
        def remove_stopwords(text, stop_words=stopword):
            tokens = [w for w in tokenize_text(text) if w not in stop_words]
            return ' '.join(tokens)

        def stem_text(text, stemmer=default_stemmer):
            tokens = tokenize_text(text)
            return ' '.join([stemmer.stem(t) for t in tokens])

        text = stem_text(text) # stemming
        text = remove_stopwords(text) # remove stopwords
        #text.strip(' ') # strip whitespaces again?

        return text

email['content'] = email['content'].apply(clean_text)

"""***WordCloud*** to check the unwanted meaningless words..."""

wcloud = WordCloud(collocations=False,background_color='white').generate(' '.join(email['content']))
plt.imshow(wcloud)
plt.axis('off')
plt.show()

X = email['content']    # Features
y = email['label']      # Labels

"""**Feature Extraction by Tfidf Vectorizer**"""

cv = TfidfVectorizer(max_features = 10000)
X = cv.fit_transform(X) # Fit the Data

"""# **Model Building**

**Train & Test data splitting**
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

"""***Random Forest Classifier***"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=100, random_state=252)
rfc.fit(X_train,y_train)

pred_rf = rfc.predict(X_test)

"""*ROC AUC Curve*"""

from sklearn.metrics import roc_curve, auc
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, pred_rf)
roc_auc = auc(false_positive_rate, true_positive_rate)
roc_auc

"""find out **n_estimators**(**no. of trees**)"""

from sklearn.ensemble import RandomForestClassifier
n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]
train_results = []
test_results = []
for estimator in n_estimators:
   rf = RandomForestClassifier(n_estimators=estimator, n_jobs=-1)
   rf.fit(X_train, y_train)
   train_pred = rf.predict(X_train)
   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)
   roc_auc = auc(false_positive_rate, true_positive_rate)
   train_results.append(roc_auc)
   y_pred = rf.predict(X_test)
   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)
   roc_auc = auc(false_positive_rate, true_positive_rate)
   test_results.append(roc_auc)

from matplotlib.legend_handler import HandlerLine2D
line1, = plt.plot(n_estimators, train_results, label='Train AUC')
line2, = plt.plot(n_estimators, test_results, label='Test AUC')

plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
plt.ylabel('AUC score')
plt.xlabel('n_estimators')
plt.show()
 # ROC curve shows that, at 100 no. of trees it gives max. accuracy

"""# **Results**"""

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix,accuracy_score,f1_score
print(confusion_matrix(y_test, pred_rf))
print('\n')
print(classification_report(y_test,pred_rf))

print('Accuracy of Random Forest Classi:', accuracy_score(pred_rf ,y_test)*int(100))
 # 99.11%

"""### *Training & Testing Results...*"""

rfc.score(X_train,y_train)    # training Accuracy = 99.96%

rfc.score(X_test,y_test)      # testing Accuracy = 99.13%

pred_train = rfc.predict(X_train)    # train_prediction
pred_test  = rfc.predict(X_test)     # test_prediction

x_axis_labels = ["predict_Non_Abusive","predict_Abusive"]       # labels for x-axis
y_axis_labels = ["actual_Non_Abusive","actual_Abusive"]     # labels for y-axis

"""*Heat_Map for train_matrix*"""

train_matrix = confusion_matrix(y_train,pred_train)

h1 = sns.heatmap(train_matrix,annot=True, fmt="d",annot_kws={"size": 17}, cmap='viridis',cbar_kws={'label': 'My Colorbar','orientation': 'horizontal'}, xticklabels=x_axis_labels, yticklabels=y_axis_labels)
h1.set_yticklabels(h1.get_yticklabels(), rotation=0)
h1.set_title('train_confusion_matrix')

"""*Heat_Map for test_matrix*"""

test_matrix = confusion_matrix(y_test,pred_test)       # test confusion_matrix

h2 = sns.heatmap(test_matrix,annot=True, fmt="d",annot_kws={"size": 17}, cmap='viridis',cbar_kws={'label': 'My Colorbar','orientation': 'horizontal'}, xticklabels=x_axis_labels, yticklabels=y_axis_labels)
h2.set_yticklabels(h1.get_yticklabels(), rotation=0)
h2.set_title('test_confusion_matrix')

"""***Multinomial Naive Bayes Algorithm***"""

from sklearn.naive_bayes import MultinomialNB
NAb_model = MultinomialNB(alpha=0.5)
NAb_model.fit(X_train,y_train)

pred_stem = NAb_model.predict(X_test)

print (confusion_matrix(y_test,pred_stem))
print('\n')
print (classification_report(y_test,pred_stem))

print('Model accuracy score with stemming :',accuracy_score(pred_stem, y_test) *int(100))
 # 95.99%

"""***SVM Algorithm***"""

from sklearn.svm import SVC

svc = SVC(kernel='sigmoid', gamma=1.0)
svc.fit(X_train,y_train)
y_pred = svc.predict(X_test)

print(confusion_matrix(y_test, y_pred))
print('\n')
print(classification_report(y_test,y_pred))

print('Accuarcy of SVM_Classi:', accuracy_score(y_pred,y_test)*int(100))
 # 98.50%

"""***Decision Tree Classifier***"""

from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier(min_samples_split=5, random_state=252)
dtc.fit(X_train,y_train)

pred_dtc = dtc.predict(X_test)

print(confusion_matrix(y_test,pred_dtc))
print('\n')
print(classification_report(y_test,pred_dtc))

print('Accuracy Decision_Tree_Classi:', accuracy_score(y_test,pred_dtc)*int(100))
 # 98.80

"""***Extra Tree Classsifier***"""

from sklearn.ensemble import ExtraTreesClassifier

etc = ExtraTreesClassifier(n_estimators=70, random_state=252)
etc.fit(X_train,y_train)

pred_etc = etc.predict(X_test)

print(confusion_matrix(y_test,pred_etc))
print('\n')
print(classification_report(y_test,pred_etc))

print('Accuracy Extra_tree_Classi:', accuracy_score(y_test,pred_etc)*int(100))
 # 99.20%

"""***Bagging Classifier***"""

from sklearn.ensemble import BaggingClassifier

bc = BaggingClassifier(n_estimators=20, random_state=252)
bc.fit(X_train,y_train)

pred_bc = bc.predict(X_test)

print(confusion_matrix(y_test,pred_bc))
print('\n')
print(classification_report(y_test,pred_bc))

print('Accuracy Bagging_Classi:', accuracy_score(y_test,pred_bc)*int(100))
 # 99.00%